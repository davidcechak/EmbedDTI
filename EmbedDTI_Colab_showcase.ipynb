{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmbedDTI_Colab_showcase.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidcechak/EmbedDTI/blob/master/EmbedDTI_Colab_showcase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages setup"
      ],
      "metadata": {
        "id": "NZXcqTWsh5wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit --quiet\n",
        "!pip install torch_geometric --quiet"
      ],
      "metadata": {
        "id": "XupTuBXrmohS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX torch-sparse bug https://stackoverflow.com/a/71764992\n",
        "import torch \n",
        "print(torch.__version__)\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster --yes --quiet\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTGeusNXn_Fo",
        "outputId": "e6a9b782-567c-4b52-ab70-3b532f21a9aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n",
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 35.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 31.8 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/davidcechak/EmbedDTI.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdSKTYCqiI-I",
        "outputId": "b1e017cd-c3e3-430f-ccd2-73edfddf94dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EmbedDTI' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ./EmbedDTI\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4PUjihAs5PK",
        "outputId": "af729c73-f246-4ec4-d4df-8e0f0ee2884d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./EmbedDTI\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: EmbedDTI\n",
            "  Building wheel for EmbedDTI (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EmbedDTI: filename=EmbedDTI-1.0-py3-none-any.whl size=974 sha256=c6c105371e41a3cc7445032274962714a20cd11cefd823ae70290a546e1a0392\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xrt6p8xq/wheels/7e/30/1e/54f87249fa87fe60f0a3c489b1e5dd64a5e915e0e097f68f83\n",
            "Successfully built EmbedDTI\n",
            "Installing collected packages: EmbedDTI\n",
            "  Attempting uninstall: EmbedDTI\n",
            "    Found existing installation: EmbedDTI 1.0\n",
            "    Uninstalling EmbedDTI-1.0:\n",
            "      Successfully uninstalled EmbedDTI-1.0\n",
            "Successfully installed EmbedDTI-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage example"
      ],
      "metadata": {
        "id": "Jab3HnPZh749"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flavopereirin_smiles_1 = 'CCc(c1)ccc2[n+]1ccc3c2[nH]c4c3cccc4'\n",
        "flavopereirin_smiles_2 = 'CCc1c[n+]2ccc3c4ccccc4[nH]c3c2cc1'\n",
        "\n",
        "propionic_acid_smiles_1 = '[CH3][CH2]C(O)=O'\n",
        "propionic_acid_smiles_2 = 'CCC(=O)O'"
      ],
      "metadata": {
        "id": "nJzw4qY_mpp3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from EmbedDTI.drug_preprocess import embedDTI_smile_preprocess\n",
        "#TODO set test_MolTSmiles to optional argument\n",
        "atom_emb_1, clique_emb_1 = embedDTI_smile_preprocess(flavopereirin_smiles_1, test_MolToSmiles=True)\n",
        "atom_emb_2, clique_emb_2 = embedDTI_smile_preprocess(flavopereirin_smiles_2, test_MolToSmiles=True)\n"
      ],
      "metadata": {
        "id": "xsT5Rb2Mhzlz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Atom emb check')\n",
        "for i in range(len(atom_emb_1[1])):\n",
        "  atom_emb_are_equal = all(atom_emb_1[1][i]==atom_emb_2[1][i])\n",
        "  print(atom_emb_are_equal)\n",
        "  \n",
        "print('Clique emb check')\n",
        "for i in range(len(clique_emb_1[1])):\n",
        "  clique_emb_are_equal = all(clique_emb_1[1][i]==clique_emb_2[1][i])\n",
        "  print(clique_emb_are_equal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc1Fr6RTnOGD",
        "outputId": "41a9687d-ed41-4ef3-83f7-44489f27ad4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Atom emb check\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "Clique emb check\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "atom_num_of_nodes, atom_node_features, atom_edges = atom_emb_1\n",
        "clique_num_of_nodes, clique_node_features, clique_edges = clique_emb_1"
      ],
      "metadata": {
        "id": "P9yWUVNtkkeY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "atom_num_of_nodes\n",
        "np.array(atom_node_features).shape\n",
        "# atom_edges #Split to two separate lists\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFORSjmplRkQ",
        "outputId": "be3c03f1-ed2c-4eb0-f2ec-ed2acf7431ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19, 101)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}